#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import time
import json
import requests
import tempfile
from dataclasses import dataclass, asdict
from typing import Dict, Any, List, Optional
from urllib.parse import quote
from pathlib import Path

from dotenv import load_dotenv
from transformers import AutoTokenizer, PreTrainedTokenizerFast
from huggingface_hub import hf_hub_download

# ========= ENV =========
load_dotenv()
from huggingface_hub import login, whoami
import os

HF_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
login(token=HF_TOKEN, add_to_git_credential=True)  # —Å–æ—Ö—Ä–∞–Ω–∏—Ç –≤ git-credentials, —É–¥–æ–±–Ω–æ –¥–ª—è git-lfs
print(whoami())  # –±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —Ç–æ–∫–µ–Ω —Ä–∞–±–æ—á–∏–π

assert HF_TOKEN, "Add HUGGINGFACEHUB_API_TOKEN=hf_*** to your .env"

# ========= MODELS =========
# –ü–æ –∑–∞–ø—Ä–æ—Å—É: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–º–µ–Ω–Ω–æ —ç—Ç–∏ –¥–≤–µ –º–æ–¥–µ–ª–∏
MODELS = [
    "meta-llama/Llama-3.2-1B-Instruct",
    "Qwen/Qwen2.5-7B-Instruct",
]

# ========= PROMPT =========
PROMPT = (
    "–û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
    "–ó–∞–¥–∞—á–∞: –æ–±—ä—è—Å–Ω–∏ —Ä–∞–∑–Ω–∏—Ü—É –º–µ–∂–¥—É zero-shot, few-shot –∏ fine-tuning. "
    "–ö–æ–≥–¥–∞ —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏ –ø–æ—á–µ–º—É? –ö–æ—Ä–æ—Ç–∫–æ, –Ω–æ –ø–æ –¥–µ–ª—É."
)

# ========= GEN PARAMS =========
GEN_PARAMS = {
    "max_new_tokens": 320,
    "temperature": 0.2,
    "top_p": 0.9,
    "repetition_penalty": 1.05,
}

# ========= PRICING (optional) =========
# –∑–Ω–∞—á–µ–Ω–∏—è ‚Äî $ –∑–∞ 1k —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–º–µ—Ä; –ø–æ–¥—Å—Ç–∞–≤—å —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–∞—Ä–∏—Ñ—ã —Ç–≤–æ–µ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞)
COSTS_PER_1K = {
    # "meta-llama/Llama-3.2-1B-Instruct": {"input": 0.2, "output": 0.6},
    # "Qwen/Qwen2.5-7B-Instruct": {"input": 0.15, "output": 0.45},
}

# ========= HF Router endpoints =========
ROUTER_MODEL_URL_TMPL = "https://router.huggingface.co/hf-inference/models/{model}"
ROUTER_CHAT_URL = "https://router.huggingface.co/v1/chat/completions"

HEADERS_JSON = {
    "Authorization": f"Bearer {HF_TOKEN}",
    "Content-Type": "application/json",
    "Accept": "application/json",
}

# ========= TOKENIZER FILE MAP (–¥–ª—è —Ä—É—á–Ω–æ–π –ø–æ–¥–≥—Ä—É–∑–∫–∏) =========
# Qwen –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç—É–ø–µ–Ω –±–µ–∑ –ø—Ä–æ–±–ª–µ–º; –¥–ª—è Llama ‚Äî gated, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–±—É–µ–º —Å–∫–∞—á–∞—Ç—å,
# –∏–Ω–∞—á–µ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ GPT-2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.
TOKENIZER_FILES: Dict[str, List[str]] = {
    "meta-llama/Llama-3.2-1B-Instruct": [
        "tokenizer.json",
        "tokenizer_config.json",
        "special_tokens_map.json",
    ],
    "Qwen/Qwen2.5-7B-Instruct": [
        "tokenizer.json",
        "tokenizer_config.json",
        # —É –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –µ—Å—Ç—å merges/vocab, –Ω–æ fast-–≤–µ—Ä—Å–∏—è –∏–∑ tokenizer.json –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –∫–µ–π—Å
    ],
}

# ========= DATA CLASSES =========
@dataclass
class RunResult:
    model: str
    latency_sec: float
    input_tokens: int
    output_tokens: int
    total_tokens: int
    cost_usd: float
    text: str
    error: str = ""


# ========= TOKENIZER HELPERS =========
def try_download_tokenizer_dir(model: str) -> Optional[Path]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è —Å–∫–∞—á–∞—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ñ–∞–π–ª—ã —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –ø–∞–ø–∫–µ –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, gated).
    """
    files = TOKENIZER_FILES.get(model)
    if not files:
        return None
    tmpdir = Path(tempfile.mkdtemp(prefix="tok_"))
    try:
        for fname in files:
            hf_hub_download(
                repo_id=model,
                filename=fname,
                local_dir=tmpdir,
                local_dir_use_symlinks=False,
                token=HF_TOKEN,
            )
        return tmpdir
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª—ã —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ {model}: {e}")
        return None


def get_tokenizer(model: str) -> PreTrainedTokenizerFast:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä:
    1) –ü—ã—Ç–∞–µ—Ç—Å—è –∞–≤—Ç–æ-—Å–∫–∞—á–∞—Ç—å –∏–∑ —Å–∞–º–æ–≥–æ —Ä–µ–ø–æ.
    2) –ï—Å–ª–∏ –Ω–µ –≤—ã—à–ª–æ (gated), –ø—ã—Ç–∞–µ—Ç—Å—è —Å–∫–∞—á–∞—Ç—å –Ω—É–∂–Ω—ã–µ —Ñ–∞–π–ª—ã –≤—Ä—É—á–Ω—É—é –≤ temp.
    3) –ï—Å–ª–∏ –∏ —ç—Ç–æ –Ω–µ —É–¥–∞–ª–æ—Å—å ‚Äî –ø–∞–¥–∞–µ—Ç –Ω–∞ GPT-2 –∫–∞–∫ –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π fallback (–¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞).
    """
    # –ü–æ–ø—ã—Ç–∫–∞ –æ–±—ã—á–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
    try:
        tok = AutoTokenizer.from_pretrained(model, use_fast=True, trust_remote_code=False)
        _print_tok_info(model, tok, source="auto")
        return tok
    except Exception as auto_err:
        print(f"‚ö†Ô∏è –ê–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –¥–ª—è {model} –Ω–µ —É–¥–∞–ª–∞—Å—å: {auto_err}")

    # –ü–æ–ø—ã—Ç–∫–∞ —Ä—É—á–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤
    local_dir = try_download_tokenizer_dir(model)
    if local_dir:
        try:
            tok = AutoTokenizer.from_pretrained(str(local_dir), use_fast=True, trust_remote_code=False)
            _print_tok_info(model, tok, source=f"local:{local_dir}")
            return tok
        except Exception as local_err:
            print(f"‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –¥–ª—è {model} –Ω–µ —É–¥–∞–ª–∞—Å—å: {local_err}")

    # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback ‚Äî GPT-2 (–ø–æ–¥—Å—á—ë—Ç –±—É–¥–µ—Ç ‚âà, –Ω–æ –ª—É—á—à–µ, —á–µ–º –Ω–∏—á–µ–≥–æ)
    tok = AutoTokenizer.from_pretrained("gpt2", use_fast=True)
    _print_tok_info(model, tok, source="fallback:gpt2")
    return tok


def _print_tok_info(model: str, tok: PreTrainedTokenizerFast, source: str):
    try:
        print(
            f"üî§ [{model}] tokenizer={tok.__class__.__name__} | "
            f"source={source} | vocab={getattr(tok, 'vocab_size', '?')} | "
            f"max_ctx={getattr(tok, 'model_max_length', '?')}"
        )
    except Exception:
        pass


def num_tokens(model: str, text: str, cache: Dict[str, PreTrainedTokenizerFast]) -> int:
    """
    –°—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ —Å—Ç—Ä–æ–≥–æ —Ç–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º, –∫–æ—Ç–æ—Ä—ã–π –º—ã —Ä–µ—à–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏.
    """
    if model not in cache:
        cache[model] = get_tokenizer(model)
    tok = cache[model]
    try:
        return len(tok.encode(text, add_special_tokens=False))
    except Exception:
        # –µ—Å–ª–∏ –¥–∞–∂–µ –∑–¥–µ—Å—å —á—Ç–æ-—Ç–æ –ø–æ–π–¥—ë—Ç –Ω–µ —Ç–∞–∫ ‚Äî –≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞
        return max(1, len(text.split()))


def estimate_cost_usd(model: str, input_toks: int, output_toks: int) -> float:
    pricing = COSTS_PER_1K.get(model) or {"input": 0.0, "output": 0.0}
    return (input_toks / 1000.0) * pricing["input"] + (output_toks / 1000.0) * pricing["output"]


# ========= HTTP HELPERS =========
def _safe_json(resp: requests.Response) -> Optional[Any]:
    try:
        return resp.json()
    except Exception:
        return None


def _router_model_inference(model: str, prompt: str) -> Dict[str, Any]:
    """
    Classic HF Router: POST /hf-inference/models/{model} c inputs.
    """
    model_path = quote(model, safe="/")
    url = ROUTER_MODEL_URL_TMPL.format(model=model_path)
    payload = {
        "inputs": prompt,
        "parameters": GEN_PARAMS,
        "options": {"use_cache": True, "wait_for_model": True},
    }
    try:
        resp = requests.post(url, headers=HEADERS_JSON, data=json.dumps(payload), timeout=180)
    except Exception as e:
        return {"ok": False, "error": f"Request error (classic): {e}"}

    data = _safe_json(resp)
    if resp.status_code == 200 and data is not None:
        return {"ok": True, "data": data}
    if data is None:
        return {"ok": False, "error": f"Non-JSON response (classic). HTTP {resp.status_code}", "status": resp.status_code, "raw": resp.text[:400]}
    return {"ok": False, "error": f"HTTP {resp.status_code} (classic): {data}", "status": resp.status_code, "raw": data}


def _router_chat_completions(model: str, prompt: str) -> Dict[str, Any]:
    """
    Fallback: OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π /v1/chat/completions.
    """
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt},
        ],
        "temperature": GEN_PARAMS.get("temperature", 0.7),
        "top_p": GEN_PARAMS.get("top_p", 1.0),
        "max_tokens": GEN_PARAMS.get("max_new_tokens", 256),
    }
    try:
        resp = requests.post(ROUTER_CHAT_URL, headers=HEADERS_JSON, data=json.dumps(payload), timeout=180)
    except Exception as e:
        return {"ok": False, "error": f"Request error (chat): {e}"}

    data = _safe_json(resp)
    if resp.status_code == 200 and data is not None:
        return {"ok": True, "data": data}
    if data is None:
        return {"ok": False, "error": f"Non-JSON response (chat). HTTP {resp.status_code}", "status": resp.status_code, "raw": resp.text[:400]}
    return {"ok": False, "error": f"HTTP {resp.status_code} (chat): {data}", "status": resp.status_code, "raw": data}


def call_hf(model: str, prompt: str, retries: int = 3, backoff: float = 5.0) -> Dict[str, Any]:
    """
    –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º classic Router ‚Üí –µ—Å–ª–∏ –Ω–µ –≤–∑–ª–µ—Ç–∞–µ—Ç, –ø—Ä–æ–±—É–µ–º chat-completions (–æ–±–∞ —Å —Ä–µ—Ç—Ä–∞—è–º–∏).
    """
    last_err = None
    # classic
    for attempt in range(1, retries + 1):
        r = _router_model_inference(model, prompt)
        if r.get("ok"):
            return {"mode": "classic", "data": r["data"]}
        last_err = r
        if attempt < retries:
            time.sleep(backoff * attempt)

    # chat fallback
    for attempt in range(1, retries + 1):
        r = _router_chat_completions(model, prompt)
        if r.get("ok"):
            return {"mode": "chat", "data": r["data"]}
        last_err = r
        if attempt < retries:
            time.sleep(backoff * attempt)

    return {"mode": "none", "error": last_err.get("error", "Unknown error"), "raw": last_err.get("raw")}


# ========= PARSERS =========
def extract_text_from_classic(data: Any) -> str:
    if isinstance(data, list) and data:
        item = data[0]
        if isinstance(item, dict):
            if "generated_text" in item and isinstance(item["generated_text"], str):
                return item["generated_text"]
            for k in ["output_text", "text", "content"]:
                if k in item and isinstance(item[k], str):
                    return item[k]
    if isinstance(data, dict):
        for k in ["generated_text", "text", "output_text", "content"]:
            if k in data and isinstance(data[k], str):
                return data[k]
    return ""


def extract_text_from_chat(data: Dict[str, Any]) -> str:
    try:
        choices = data.get("choices") or []
        if choices and "message" in choices[0]:
            return choices[0]["message"].get("content", "") or ""
    except Exception:
        pass
    return ""


def extract_text(mode: str, data: Any) -> str:
    if mode == "classic":
        return extract_text_from_classic(data)
    if mode == "chat":
        return extract_text_from_chat(data)
    return ""


# ========= MAIN RUN =========
def run_once(model: str, prompt: str, tcache: Dict[str, PreTrainedTokenizerFast]) -> RunResult:
    input_toks = num_tokens(model, prompt, tcache)

    t0 = time.perf_counter()
    resp = call_hf(model, prompt)
    dt = round(time.perf_counter() - t0, 3)

    if "error" in resp and resp.get("mode") == "none":
        return RunResult(
            model=model,
            latency_sec=dt,
            input_tokens=input_toks,
            output_tokens=0,
            total_tokens=input_toks,
            cost_usd=estimate_cost_usd(model, input_toks, 0),
            text="",
            error=f"{resp.get('error')} | raw: {str(resp.get('raw'))[:180]}",
        )

    mode = resp.get("mode", "classic")
    data = resp.get("data")
    text = extract_text(mode, data)
    if not text:
        return RunResult(
            model=model,
            latency_sec=dt,
            input_tokens=input_toks,
            output_tokens=0,
            total_tokens=input_toks,
            cost_usd=estimate_cost_usd(model, input_toks, 0),
            text="",
            error=f"Cannot parse response ({mode}). Snippet: {str(data)[:180]}",
        )

    output_toks = num_tokens(model, text, tcache)
    total = input_toks + output_toks
    cost = estimate_cost_usd(model, input_toks, output_toks)

    return RunResult(
        model=model,
        latency_sec=dt,
        input_tokens=input_toks,
        output_tokens=output_toks,
        total_tokens=total,
        cost_usd=round(cost, 6),
        text=text.strip(),
        error=""
    )


def pretty_print(results: List[RunResult]):
    header = f"{'Model':40s} {'Latency(s)':>10s} {'In':>6s} {'Out':>6s} {'Total':>7s} {'Cost($)':>8s}"
    print(header)
    print("-" * len(header))
    for r in results:
        print(f"{r.model[:40]:40s} {r.latency_sec:10.3f} {r.input_tokens:6d} {r.output_tokens:6d} {r.total_tokens:7d} {r.cost_usd:8.4f}")
        if r.error:
            print(f"   ERROR: {r.error}")
    print("\n‚ñº –ö—Ä–∞—Ç–∫–∏–π –≤—ã–≤–æ–¥ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É (–æ—Ü–µ–Ω–∫–∞ –≤—Ä—É—á–Ω—É—é):\n"
          "   - –°–º–æ—Ç—Ä–∏ —Ç–æ—á–Ω–æ—Å—Ç—å, —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –ø—Ä–∏–º–µ—Ä—ã, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π.\n")


def main():
    tcache: Dict[str, PreTrainedTokenizerFast] = {}
    results: List[RunResult] = []

    for m in MODELS:
        print(f"==> Running: {m}")
        res = run_once(m, PROMPT, tcache)
        results.append(res)

    pretty_print(results)

    out = [asdict(r) for r in results]
    with open("results.json", "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

    print("\n=== –û—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–µ–π ===\n")
    for r in results:
        print(f"[{r.model}]")
        if r.error:
            print(f"ERROR: {r.error}\n")
        else:
            print(r.text + "\n")


if __name__ == "__main__":
    main()

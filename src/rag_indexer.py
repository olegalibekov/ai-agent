# rag_indexer.py

import requests
import numpy as np
import faiss
import pickle
import json
from typing import List, Dict
from pathlib import Path


class OllamaRAG:
    def __init__(self,
                 embedding_model='nomic-embed-text',
                 ollama_url='http://localhost:11434'):
        self.embedding_model = embedding_model
        self.ollama_url = ollama_url
        self.index = None
        self.chunks = []
        self.metadata = []

    def get_embedding(self, text: str) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Ollama"""
        try:
            response = requests.post(
                f'{self.ollama_url}/api/embeddings',  # üëà –≤–º–µ—Å—Ç–æ /api/embeddings
                json={
                    'model': self.embedding_model,
                    'input': text,  # üëà –≤–º–µ—Å—Ç–æ prompt
                },
                timeout=60,
            )

            if response.status_code != 200:
                print(f"‚ùå Ollama –≤–µ—Ä–Ω—É–ª–∞ –æ—à–∏–±–∫—É {response.status_code}")
                print("–û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞:")
                print(response.text)
                response.raise_for_status()

            data = response.json()

            if 'embedding' not in data:
                print("‚ùå –û—à–∏–±–∫–∞: –≤ –æ—Ç–≤–µ—Ç–µ –Ω–µ—Ç –ø–æ–ª—è 'embedding'")
                print("–ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç:", data)
                raise KeyError("'embedding' not found in response")

            return np.array(data['embedding'], dtype=np.float32)

        except requests.exceptions.RequestException as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: `ollama serve`")
            raise
        except KeyError:
            print(f"‚ùå –ú–æ–¥–µ–ª—å {self.embedding_model} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç embeddings –∏–ª–∏ –æ—Ç–≤–µ—Ç –¥—Ä—É–≥–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞.")
            raise

    def chunk_text(self, text: str, chunk_size=1000, overlap=100) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º (500-1000 —Ç–æ–∫–µ–Ω–æ–≤)"""
        char_per_token = 4
        chunk_chars = chunk_size * char_per_token
        overlap_chars = overlap * char_per_token

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_chars
            chunk = text[start:end]

            if end < len(text):
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                split_point = max(last_period, last_newline)

                if split_point > len(chunk) * 0.5:
                    chunk = chunk[:split_point + 1]
                    end = start + split_point + 1

            if chunk.strip():
                chunks.append(chunk.strip())

            start = end - overlap_chars

        return chunks

    def add_documents(self, documents: List[Dict[str, str]]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å"""
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        all_embeddings = []

        for doc in documents:
            print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞: {doc['source']}")
            chunks = self.chunk_text(doc['text'], chunk_size=800, overlap=80)
            print(f"  –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")

            for i, chunk in enumerate(chunks):
                embedding = self.get_embedding(chunk)
                all_embeddings.append(embedding)

                self.chunks.append(chunk)
                self.metadata.append({
                    'source': doc['source'],
                    'chunk_id': i,
                    'total_chunks': len(chunks)
                })

                if (i + 1) % 10 == 0:
                    print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1}/{len(chunks)} —á–∞–Ω–∫–æ–≤")

        embeddings_array = np.array(all_embeddings).astype('float32')
        norms = np.linalg.norm(embeddings_array, axis=1, keepdims=True)
        embeddings_array = embeddings_array / norms

        dimension = embeddings_array.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(embeddings_array)

        print(f"\n‚úì –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {len(self.chunks)} —á–∞–Ω–∫–æ–≤, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {dimension}")

    def search(self, query: str, top_k=5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        if self.index is None:
            raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–∑–¥–∞–Ω. –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ add_documents() –∏–ª–∏ load()")

        query_embedding = self.get_embedding(query)
        query_embedding = query_embedding / np.linalg.norm(query_embedding)

        distances, indices = self.index.search(
            query_embedding.reshape(1, -1).astype('float32'),
            top_k
        )

        results = []
        for score, idx in zip(distances[0], indices[0]):
            results.append({
                'text': self.chunks[idx],
                'metadata': self.metadata[idx],
                'score': float(score)
            })

        return results

    def save(self, path='rag_index'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        Path(path).mkdir(exist_ok=True)
        faiss.write_index(self.index, f'{path}/vectors.faiss')

        with open(f'{path}/data.pkl', 'wb') as f:
            pickle.dump({
                'chunks': self.chunks,
                'metadata': self.metadata,
                'config': {
                    'embedding_model': self.embedding_model,
                    'total_chunks': len(self.chunks)
                }
            }, f)

        print(f"‚úì –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {path}/")

    def load(self, path='rag_index'):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        self.index = faiss.read_index(f'{path}/vectors.faiss')

        with open(f'{path}/data.pkl', 'rb') as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.metadata = data['metadata']

        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å: {len(self.chunks)} —á–∞–Ω–∫–æ–≤")


def load_documents_from_folder(folder_path: str) -> List[Dict[str, str]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏"""
    documents = []
    folder = Path(folder_path)

    if not folder.exists():
        print(f"‚ö† –ü–∞–ø–∫–∞ {folder_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return documents

    for ext in ['*.md', '*.txt']:
        for file_path in folder.glob(ext):
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ {file_path.name}...")
            with open(file_path, 'r', encoding='utf-8') as f:
                documents.append({
                    'text': f.read(),
                    'source': file_path.name
                })

    for file_path in folder.glob('*.py'):
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ {file_path.name}...")
        with open(file_path, 'r', encoding='utf-8') as f:
            documents.append({
                'text': f.read(),
                'source': file_path.name
            })

    try:
        import pdfplumber
        for file_path in folder.glob('*.pdf'):
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ {file_path.name}...")
            with pdfplumber.open(file_path) as pdf:
                text = '\n'.join(page.extract_text() or '' for page in pdf.pages)
                documents.append({
                    'text': text,
                    'source': file_path.name
                })
    except ImportError:
        pass

    print(f"\n‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    return documents


def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
    print("=" * 80)
    print("RAG –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("=" * 80)

    rag = OllamaRAG()

    docs_folder = input("\n–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é './documents'): ").strip()
    if not docs_folder:
        docs_folder = '/Users/fehty/PycharmProjects/ai-agent/documents/'

    documents = load_documents_from_folder(docs_folder)

    if not documents:
        print("\n‚ùå –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
        return

    print("\n" + "=" * 80)
    print("–ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
    print("=" * 80)

    try:
        rag.add_documents(documents)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return

    index_path = input("\n–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'rag_index'): ").strip()
    if not index_path:
        index_path = 'rag_index'

    rag.save(index_path)

    print("\n" + "=" * 80)
    print("–¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫")
    print("=" * 80)

    while True:
        query = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
        if query.lower() in ['exit', 'quit', '–≤—ã—Ö–æ–¥']:
            break

        if not query:
            continue

        results = rag.search(query, top_k=3)

        print("\n" + "-" * 80)
        print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:")
        print("-" * 80)

        for i, result in enumerate(results, 1):
            print(f"\n{i}. –ò—Å—Ç–æ—á–Ω–∏–∫: {result['metadata']['source']}")
            print(f"   –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['score']:.4f}")
            print(f"   –ß–∞–Ω–∫: {result['metadata']['chunk_id'] + 1}/{result['metadata']['total_chunks']}")
            print(f"\n   {result['text'][:400]}...")

    print("\n‚úì –ì–æ—Ç–æ–≤–æ!")


if __name__ == '__main__':
    main()
"""
News RAG System
–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
"""
import os
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import json

class NewsRAG:
    def __init__(self, index_path: str = "data/news_index"):
        self.index_path = Path(index_path)
        self.index_path.mkdir(parents=True, exist_ok=True)
        
        self.model = None
        self.index = None
        self.news_items = []
        
    def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞"""
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã...")
        self.model = SentenceTransformer('all-mpnet-base-v2')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ –µ—Å—Ç—å
        self._load_index()
        
        print("‚úì RAG —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞")
    
    def _load_index(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å"""
        index_file = self.index_path / "news.index"
        data_file = self.index_path / "news_data.json"
        
        if index_file.exists() and data_file.exists():
            try:
                self.index = faiss.read_index(str(index_file))
                
                with open(data_file, 'r', encoding='utf-8') as f:
                    self.news_items = json.load(f)
                
                print(f"  ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.news_items)} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ –∏–Ω–¥–µ–∫—Å–∞")
            except Exception as e:
                print(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
                self._create_empty_index()
        else:
            self._create_empty_index()
    
    def _create_empty_index(self):
        """–°–æ–∑–¥–∞—ë—Ç –ø—É—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å"""
        dimension = 768  # all-mpnet-base-v2
        self.index = faiss.IndexFlatL2(dimension)
        self.news_items = []
        print("  ‚úì –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å")
    
    def _save_index(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å"""
        index_file = self.index_path / "news.index"
        data_file = self.index_path / "news_data.json"
        
        faiss.write_index(self.index, str(index_file))
        
        with open(data_file, 'w', encoding='utf-8') as f:
            json.dump(self.news_items, f, ensure_ascii=False, indent=2)
    
    def add_news(self, news_item: Dict):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤–æ—Å—Ç—å –≤ –∏–Ω–¥–µ–∫—Å"""
        # –°–æ–∑–¥–∞—ë–º —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        text = f"{news_item['title']} {news_item.get('description', '')}"
        
        # –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        embedding = self.model.encode([text])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ FAISS
        self.index.add(embedding.astype('float32'))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        news_item['added_at'] = datetime.utcnow().isoformat()
        self.news_items.append(news_item)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        self._save_index()
    
    def check_duplicate(self, title: str, description: str = "", 
                       similarity_threshold: float = 0.8) -> Optional[Dict]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–º
        
        Returns:
            Dict —Å –ø–æ—Ö–æ–∂–µ–π –Ω–æ–≤–æ—Å—Ç—å—é –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç, –∏–Ω–∞—á–µ None
        """
        if self.index.ntotal == 0:
            return None
        
        # –°–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –Ω–æ–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏
        text = f"{title} {description}"
        query_embedding = self.model.encode([text])
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
        distances, indices = self.index.search(
            query_embedding.astype('float32'), 
            min(5, self.index.ntotal)
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º similarity (–º–µ–Ω—å—à–µ distance = –±–æ–ª—å—à–µ –ø–æ—Ö–æ–∂–µ—Å—Ç—å)
        for distance, idx in zip(distances[0], indices[0]):
            if idx < len(self.news_items):
                similarity = 1.0 / (1.0 + distance)
                
                if similarity >= similarity_threshold:
                    similar_news = self.news_items[idx].copy()
                    similar_news['similarity'] = float(similarity)
                    return similar_news
        
        return None
    
    def get_recent_news(self, hours: int = 24) -> List[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours)
        
        recent = []
        for item in self.news_items:
            added_at = datetime.fromisoformat(item['added_at'])
            if added_at >= cutoff_time:
                recent.append(item)
        
        return recent
    
    def get_trending_topics(self, hours: int = 24, top_k: int = 5) -> List[str]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç–µ–º—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤"""
        recent = self.get_recent_news(hours)
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Å—á—ë—Ç —Å–ª–æ–≤ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
        word_counts = {}
        
        for item in recent:
            words = item['title'].lower().split()
            for word in words:
                if len(word) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                    word_counts[word] = word_counts.get(word, 0) + 1
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        
        return [word for word, count in sorted_words[:top_k]]
    
    def get_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        total = len(self.news_items)
        
        if total == 0:
            return {
                "total_news": 0,
                "last_24h": 0,
                "last_7d": 0
            }
        
        last_24h = len(self.get_recent_news(24))
        last_7d = len(self.get_recent_news(24 * 7))
        
        return {
            "total_news": total,
            "last_24h": last_24h,
            "last_7d": last_7d,
            "trending_topics": self.get_trending_topics(24, 3)
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    rag = NewsRAG()
    rag.initialize()
    
    # –¢–µ—Å—Ç–æ–≤–∞—è –Ω–æ–≤–æ—Å—Ç—å
    test_news = {
        "title": "Apple –∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª–∞ –Ω–æ–≤—ã–π iPhone 16",
        "description": "–ö–æ–º–ø–∞–Ω–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ —Ñ–ª–∞–≥–º–∞–Ω—Å–∫–∏–π —Å–º–∞—Ä—Ç—Ñ–æ–Ω —Å –Ω–æ–≤—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏",
        "url": "https://example.com/news/1",
        "source": "TechCrunch"
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç
    duplicate = rag.check_duplicate(test_news['title'], test_news['description'])
    
    if duplicate:
        print(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç (similarity: {duplicate['similarity']:.2f})")
        print(f"   –ü–æ—Ö–æ–∂–∞—è –Ω–æ–≤–æ—Å—Ç—å: {duplicate['title']}")
    else:
        print("‚úì –ù–æ–≤–æ—Å—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–∞—è, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å")
        rag.add_news(test_news)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = rag.get_stats()
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {stats['total_news']}")
    print(f"   –ó–∞ 24 —á–∞—Å–∞: {stats['last_24h']}")

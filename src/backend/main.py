"""
Team Assistant Backend
RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –∑–Ω–∞–Ω–∏–π –æ –ø—Ä–æ–µ–∫—Ç–µ
"""
import os
from pathlib import Path
from typing import List, Dict, Optional
from contextlib import asynccontextmanager
import anthropic
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

from dotenv import load_dotenv

load_dotenv()

# ============================================================================
# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
# ============================================================================

class QueryRequest(BaseModel):
    query: str
    context: Optional[Dict] = None

class IndexRequest(BaseModel):
    kb_path: str

# ============================================================================
# RAG —Å–∏—Å—Ç–µ–º–∞
# ============================================================================

class TeamRAG:
    def __init__(self):
        self.model = None
        self.index = None
        self.documents = []
        self.embeddings = None
        self.anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
        
        if not self.anthropic_api_key:
            print("‚ö†Ô∏è ANTHROPIC_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã...")
        self.model = SentenceTransformer('all-mpnet-base-v2')
        print("‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    def load_knowledge_base(self, kb_path: str) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        kb_path = Path(kb_path)
        documents = []
        
        print(f"üìö –ó–∞–≥—Ä—É–∂–∞—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ {kb_path}...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
        for file_path in kb_path.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.md', '.py', '.txt']:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
                    file_type = "code" if file_path.suffix == '.py' else "docs"
                    
                    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
                    chunks = self._split_content(content, str(file_path.name), file_type)
                    documents.extend(chunks)
                    
                    print(f"  ‚úì {file_path.relative_to(kb_path)}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
                except Exception as e:
                    print(f"  ‚úó –û—à–∏–±–∫–∞ {file_path.name}: {e}")
        
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤")
        return documents
    
    def _split_content(self, content: str, filename: str, file_type: str) -> List[Dict]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        
        if file_type == "code":
            # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ñ—É–Ω–∫—Ü–∏—è–º/–∫–ª–∞—Å—Å–∞–º
            lines = content.split('\n')
            current_chunk = []
            current_name = filename
            
            for line in lines:
                if line.startswith('def ') or line.startswith('class '):
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —á–∞–Ω–∫
                    if current_chunk:
                        chunk_text = '\n'.join(current_chunk).strip()
                        if len(chunk_text) > 50:
                            chunks.append({
                                'text': chunk_text,
                                'source': filename,
                                'type': 'code',
                                'name': current_name
                            })
                    
                    # –ù–æ–≤—ã–π —á–∞–Ω–∫
                    current_name = line.split('(')[0].replace('def ', '').replace('class ', '').strip()
                    current_chunk = [line]
                else:
                    current_chunk.append(line)
            
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if current_chunk:
                chunk_text = '\n'.join(current_chunk).strip()
                if len(chunk_text) > 50:
                    chunks.append({
                        'text': chunk_text,
                        'source': filename,
                        'type': 'code',
                        'name': current_name
                    })
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
            lines = content.split('\n')
            current_chunk = []
            current_header = filename
            
            for line in lines:
                if line.startswith('#'):
                    if current_chunk:
                        chunk_text = '\n'.join(current_chunk).strip()
                        if len(chunk_text) > 50:
                            chunks.append({
                                'text': chunk_text,
                                'source': filename,
                                'type': 'docs',
                                'header': current_header
                            })
                    
                    current_header = line.strip('#').strip()
                    current_chunk = [line]
                else:
                    current_chunk.append(line)
            
            if current_chunk:
                chunk_text = '\n'.join(current_chunk).strip()
                if len(chunk_text) > 50:
                    chunks.append({
                        'text': chunk_text,
                        'source': filename,
                        'type': 'docs',
                        'header': current_header
                    })
        
        return chunks
    
    def create_index(self, documents: List[Dict]):
        """–°–æ–∑–¥–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å"""
        print("üî® –°–æ–∑–¥–∞—é FAISS –∏–Ω–¥–µ–∫—Å...")
        
        self.documents = documents
        texts = [doc['text'] for doc in documents]
        
        # –°–æ–∑–¥–∞–µ–º embeddings
        self.embeddings = self.model.encode(texts, show_progress_bar=True)
        
        # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dimension)
        self.index.add(self.embeddings.astype('float32'))
        
        print(f"‚úì –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.index is None:
            return []
        
        # –°–æ–∑–¥–∞–µ–º embedding –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query])
        
        # –ü–æ–∏—Å–∫ –≤ FAISS
        distances, indices = self.index.search(
            query_embedding.astype('float32'), 
            top_k
        )
        
        results = []
        for idx, distance in zip(indices[0], distances[0]):
            if idx < len(self.documents):
                doc = self.documents[idx]
                results.append({
                    'text': doc['text'],
                    'source': doc['source'],
                    'type': doc['type'],
                    'name': doc.get('name') or doc.get('header', ''),
                    'score': float(distance)
                })
        
        return results
    
    def generate_answer(self, query: str, context_docs: List[Dict], 
                       project_context: Optional[Dict] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é Claude"""
        if not self.anthropic_api_key:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—ã—Ä–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_text = "\n\n---\n\n".join([
                f"**{doc['name']}** ({doc['source']})\n{doc['text'][:300]}..."
                for doc in context_docs
            ])
            return f"üìö –ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n\n{context_text}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_text = "\n\n".join([
            f"# {doc['name']} ({doc['type']})\n{doc['text']}"
            for doc in context_docs
        ])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
        project_info = ""
        if project_context:
            sprint = project_context.get('sprint', {})
            blockers = project_context.get('blockers', {})
            
            project_info = f"""
**–¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞:**
- –°–ø—Ä–∏–Ω—Ç: {sprint.get('name', 'N/A')}
- –ü—Ä–æ–≥—Ä–µ—Å—Å: {sprint.get('completion_percent', 0)}%
- –ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á: {blockers.get('blocked_tasks', 0)}
- –ë–ª–æ–∫–µ—Ä–æ–≤ —Ä–µ–ª–∏–∑–∞: {blockers.get('release_blockers', 0)}
"""
        
        # –ü—Ä–æ–º–ø—Ç
        prompt = f"""–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ–º–∞–Ω–¥—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ CloudDocs.

{project_info}

**–í–æ–ø—Ä–æ—Å:** {query}

**–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—Ä–æ–µ–∫—Ç–∞:**
{context_text}

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:**
1. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
2. –ï—Å–ª–∏ —ç—Ç–æ –≤–æ–ø—Ä–æ—Å –æ –∫–æ–¥–µ - –¥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã
3. –ï—Å–ª–∏ —ç—Ç–æ –≤–æ–ø—Ä–æ—Å –æ –∑–∞–¥–∞—á–∞—Ö - —É—á–∏—Ç—ã–≤–∞–π —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
4. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º
5. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ - —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
- –ü—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
- –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–æ–¥–∞/–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
- –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)"""

        try:
            client = anthropic.Anthropic(api_key=self.anthropic_api_key)
            message = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=2000,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return message.content[0].text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ Claude API: {e}")
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}"

# ============================================================================
# FastAPI App
# ============================================================================

rag_system = TeamRAG()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan events"""
    rag_system.initialize()
    yield

app = FastAPI(title="Team Assistant Backend", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================================
# API Endpoints
# ============================================================================

@app.get("/health")
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è"""
    return {
        "status": "ok",
        "service": "Team Assistant Backend",
        "rag_initialized": rag_system.model is not None,
        "documents_indexed": len(rag_system.documents)
    }

@app.post("/index")
async def index_knowledge_base(request: IndexRequest):
    """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
    try:
        kb_path = Path(request.kb_path)
        if not kb_path.exists():
            raise HTTPException(status_code=404, detail=f"–ü—É—Ç—å {kb_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        documents = rag_system.load_knowledge_base(kb_path)
        
        if not documents:
            raise HTTPException(status_code=400, detail="–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã")
        
        rag_system.create_index(documents)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        sources = set(doc['source'] for doc in documents)
        types = {}
        for doc in documents:
            doc_type = doc['type']
            types[doc_type] = types.get(doc_type, 0) + 1
        
        return {
            "message": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞",
            "total_chunks": len(documents),
            "sources": list(sources),
            "types": types,
            "status": "success"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/ask")
async def ask_question(request: QueryRequest):
    """–û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –æ –ø—Ä–æ–µ–∫—Ç–µ"""
    try:
        if not rag_system.index:
            raise HTTPException(
                status_code=400,
                detail="–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /index"
            )
        
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        relevant_docs = rag_system.search(request.query, top_k=5)
        
        if not relevant_docs:
            return {
                "response": "–ù–µ –Ω–∞—à–µ–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.",
                "sources": [],
                "context": []
            }
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        answer = rag_system.generate_answer(
            request.query,
            relevant_docs,
            request.context
        )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = [
            f"{doc['source']} - {doc['name']}"
            for doc in relevant_docs
        ]
        
        return {
            "response": answer,
            "sources": sources,
            "context": relevant_docs
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# –ó–∞–ø—É—Å–∫
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    
    print("=" * 60)
    print("üöÄ Team Assistant Backend")
    print("=" * 60)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)

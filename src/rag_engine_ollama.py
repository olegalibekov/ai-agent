"""
RAG Engine —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–π Ollama –º–æ–¥–µ–ª–∏
"""

import json
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

import faiss
import numpy as np
import requests


class RAGEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è RAG —Å Ollama —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""

    def __init__(self, config: dict):
        self.config = config

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.dimension = 768  # nomic-embed-text –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 768
        self.chunk_size = config.get('chunk_size', 1000)
        self.chunk_overlap = config.get('chunk_overlap', 200)
        self.top_k = config.get('top_k', 5)
        self.similarity_threshold = config.get('similarity_threshold', 0.7)

        # Ollama API
        self.ollama_url = "http://localhost:11434/api/embeddings"
        self.model_name = "nomic-embed-text"

        # –ü—É—Ç–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
        storage_path = Path(config['storage']['path'])
        storage_path.mkdir(parents=True, exist_ok=True)

        self.index_path = storage_path / "faiss_index.bin"
        self.metadata_path = storage_path / "metadata.pkl"

        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        self.index = self._load_or_create_index()
        self.metadata = self._load_metadata()

        print(f"üìö RAG Engine –∑–∞–≥—Ä—É–∂–µ–Ω —Å Ollama: {len(self.metadata)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    def _get_embedding(self, text: str) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Ollama"""
        try:
            response = requests.post(
                self.ollama_url,
                json={
                    "model": self.model_name,
                    "prompt": text
                },
                timeout=30
            )

            if response.status_code == 200:
                embedding = response.json()['embedding']
                return np.array(embedding, dtype='float32')
            else:
                print(f"–û—à–∏–±–∫–∞ Ollama API: {response.status_code}")
                return np.zeros(self.dimension, dtype='float32')

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return np.zeros(self.dimension, dtype='float32')

    def _load_or_create_index(self) -> faiss.IndexFlatL2:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        if self.index_path.exists():
            try:
                return faiss.read_index(str(self.index_path))
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")

        return faiss.IndexFlatL2(self.dimension)

    def _load_metadata(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.metadata_path.exists():
            try:
                with open(self.metadata_path, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

        return []

    def _save_index(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        try:
            faiss.write_index(self.index, str(self.index_path))
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")

    def _save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        try:
            with open(self.metadata_path, 'wb') as f:
                pickle.dump(self.metadata, f)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

    def _chunk_text(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        chunks = []
        words = text.split()

        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
            chunk = ' '.join(words[i:i + self.chunk_size])
            if chunk:
                chunks.append(chunk)

        return chunks

    async def add_document(self, text: str, metadata: Optional[Dict[str, Any]] = None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∏–Ω–¥–µ–∫—Å"""
        chunks = self._chunk_text(text)

        for chunk in chunks:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Ollama
            embedding = self._get_embedding(chunk)
            embedding = np.array([embedding]).astype('float32')

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∏–Ω–¥–µ–∫—Å
            self.index.add(embedding)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            chunk_metadata = {
                "content": chunk,
                "timestamp": datetime.now().isoformat(),
                **(metadata or {})
            }
            self.metadata.append(chunk_metadata)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self._save_index()
        self._save_metadata()

    async def add_documents(self, documents: List[Dict[str, Any]]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        for doc in documents:
            await self.add_document(
                text=doc.get('text', ''),
                metadata=doc.get('metadata')
            )

    async def search(
            self,
            query: str,
            top_k: Optional[int] = None,
            filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.index.ntotal == 0:
            return []

        k = top_k or self.top_k

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self._get_embedding(query)
        query_embedding = np.array([query_embedding]).astype('float32')

        # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
        distances, indices = self.index.search(query_embedding, min(k * 2, self.index.ntotal))

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = []
        for distance, idx in zip(distances[0], indices[0]):
            if idx == -1 or idx >= len(self.metadata):
                continue

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
            similarity = 1 / (1 + distance)
            if similarity < self.similarity_threshold:
                continue

            metadata = self.metadata[idx]

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
            if filter_metadata:
                if not all(
                        metadata.get(k) == v
                        for k, v in filter_metadata.items()
                ):
                    continue

            results.append({
                "content": metadata["content"],
                "metadata": metadata,
                "similarity": float(similarity),
                "distance": float(distance)
            })

            if len(results) >= k:
                break

        return results

    async def get_document_count(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return len(self.metadata)

    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        return {
            "total_documents": len(self.metadata),
            "index_size": self.index.ntotal,
            "dimension": self.dimension,
            "storage_path": str(self.config['storage']['path']),
            "model": "ollama/nomic-embed-text"
        }

    async def close(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
        self._save_index()
        self._save_metadata()
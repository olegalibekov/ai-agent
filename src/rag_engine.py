"""
RAG Engine –¥–ª—è God Agent
–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FAISS
"""

import json
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer


class RAGEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è Retrieval-Augmented Generation"""
    
    def __init__(self, config: dict):
        self.config = config
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.model = SentenceTransformer(
            'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
        )
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.dimension = 768
        self.chunk_size = config.get('chunk_size', 1000)
        self.chunk_overlap = config.get('chunk_overlap', 200)
        self.top_k = config.get('top_k', 5)
        self.similarity_threshold = config.get('similarity_threshold', 0.7)
        
        # –ü—É—Ç–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
        storage_path = Path(config['storage']['path'])
        storage_path.mkdir(parents=True, exist_ok=True)
        
        self.index_path = storage_path / "faiss_index.bin"
        self.metadata_path = storage_path / "metadata.pkl"
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        self.index = self._load_or_create_index()
        self.metadata = self._load_metadata()
        
        print(f"üìö RAG Engine –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.metadata)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    def _load_or_create_index(self) -> faiss.IndexFlatL2:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        if self.index_path.exists():
            try:
                return faiss.read_index(str(self.index_path))
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
        return faiss.IndexFlatL2(self.dimension)
    
    def _load_metadata(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        if self.metadata_path.exists():
            try:
                with open(self.metadata_path, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
        
        return []
    
    def _save_index(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        try:
            faiss.write_index(self.index, str(self.index_path))
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
    
    def _save_metadata(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        try:
            with open(self.metadata_path, 'wb') as f:
                pickle.dump(self.metadata, f)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def _chunk_text(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        
        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        chunks = []
        words = text.split()
        
        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):
            chunk = ' '.join(words[i:i + self.chunk_size])
            if chunk:
                chunks.append(chunk)
        
        return chunks
    
    async def add_document(
        self,
        text: str,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –∏–Ω–¥–µ–∫—Å
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self._chunk_text(text)
        
        for chunk in chunks:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            embedding = self.model.encode([chunk])[0]
            embedding = np.array([embedding]).astype('float32')
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∏–Ω–¥–µ–∫—Å
            self.index.add(embedding)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            chunk_metadata = {
                "content": chunk,
                "timestamp": datetime.now().isoformat(),
                **(metadata or {})
            }
            self.metadata.append(chunk_metadata)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self._save_index()
        self._save_metadata()
    
    async def add_documents(self, documents: List[Dict[str, Any]]):
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–ª—è–º–∏ 'text' –∏ 'metadata'
        """
        for doc in documents:
            await self.add_document(
                text=doc.get('text', ''),
                metadata=doc.get('metadata')
            )
    
    async def search(
        self,
        query: str,
        top_k: Optional[int] = None,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            filter_metadata: –§–∏–ª—å—Ç—Ä –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏
        """
        if self.index.ntotal == 0:
            return []
        
        k = top_k or self.top_k
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode([query])[0]
        query_embedding = np.array([query_embedding]).astype('float32')
        
        # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
        distances, indices = self.index.search(query_embedding, min(k * 2, self.index.ntotal))
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        results = []
        for distance, idx in zip(distances[0], indices[0]):
            if idx == -1 or idx >= len(self.metadata):
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
            similarity = 1 / (1 + distance)  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –≤ –ø–æ—Ö–æ–∂–µ—Å—Ç—å
            if similarity < self.similarity_threshold:
                continue
            
            metadata = self.metadata[idx]
            
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
            if filter_metadata:
                if not all(
                    metadata.get(k) == v
                    for k, v in filter_metadata.items()
                ):
                    continue
            
            results.append({
                "content": metadata["content"],
                "metadata": metadata,
                "similarity": float(similarity),
                "distance": float(distance)
            })
            
            if len(results) >= k:
                break
        
        return results
    
    async def delete_documents(self, filter_metadata: Dict[str, Any]):
        """
        –£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ñ–∏–ª—å—Ç—Ä—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        
        Args:
            filter_metadata: –§–∏–ª—å—Ç—Ä –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        """
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        indices_to_keep = []
        metadata_to_keep = []
        
        for idx, meta in enumerate(self.metadata):
            should_delete = all(
                meta.get(k) == v
                for k, v in filter_metadata.items()
            )
            
            if not should_delete:
                indices_to_keep.append(idx)
                metadata_to_keep.append(meta)
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        if indices_to_keep != list(range(len(self.metadata))):
            new_index = faiss.IndexFlatL2(self.dimension)
            
            for idx in indices_to_keep:
                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
                vector = faiss.rev_swig_ptr(
                    self.index.reconstruct(idx),
                    self.dimension
                )
                new_index.add(np.array([vector]).astype('float32'))
            
            self.index = new_index
            self.metadata = metadata_to_keep
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            self._save_index()
            self._save_metadata()
    
    async def clear(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        self.index = faiss.IndexFlatL2(self.dimension)
        self.metadata = []
        self._save_index()
        self._save_metadata()
    
    async def get_document_count(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return len(self.metadata)
    
    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        return {
            "total_documents": len(self.metadata),
            "index_size": self.index.ntotal,
            "dimension": self.dimension,
            "storage_path": str(self.config['storage']['path'])
        }
    
    async def close(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
        self._save_index()
        self._save_metadata()
